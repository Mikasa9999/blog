---
title: "å¼ºåŒ–å­¦ä¹ "
description: "å„è‡ªå¼ºåŒ–å­¦ä¹ å…¬å¼åŠæ€æƒ³"
pubDate: 2024-10-02
category: theory
tags: ["reinforcement-learning", "mdp", "value-based", "policy-gradient", "ppo"]
draft: false
---

# ğŸ§  å¼ºåŒ–å­¦ä¹ ç®—æ³•æ€»è§ˆï¼ˆä» DP â†’ PPO + DQNï¼‰

---

## ä¸€ã€é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰

å®šä¹‰ï¼š
$$
\mathcal{M} = (\mathcal{S}, \mathcal{A}, P, R, \gamma)
$$

ç›®æ ‡ï¼š
$$
J(\pi) = \mathbb{E}_\pi \left[ \sum_{t=0}^{\infty} \gamma^t r_t \right]
$$

çŠ¶æ€ä»·å€¼å‡½æ•°ï¼š
$$
V^\pi(s) = \mathbb{E}_\pi [ G_t | s_t = s ]
$$

åŠ¨ä½œä»·å€¼å‡½æ•°ï¼š
$$
Q^\pi(s,a) = \mathbb{E}_\pi [ G_t | s_t = s, a_t = a ]
$$

å›æŠ¥ï¼š
$$
G_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k}
$$

---

## äºŒã€åŠ¨æ€è§„åˆ’ï¼ˆDynamic Programmingï¼‰

è´å°”æ›¼æœŸæœ›æ–¹ç¨‹ï¼š
$$
V^\pi(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) [ R(s,a) + \gamma V^\pi(s') ]
$$

è´å°”æ›¼æœ€ä¼˜æ–¹ç¨‹ï¼š
$$
V^*(s) = \max_a \sum_{s'} P(s'|s,a) [ R(s,a) + \gamma V^*(s') ]
$$

åŠ¨ä½œä»·å€¼å½¢å¼ï¼š
$$
Q^*(s,a) = \sum_{s'} P(s'|s,a) [ R(s,a) + \gamma \max_{a'} Q^*(s',a') ]
$$

---

## ä¸‰ã€è’™ç‰¹å¡æ´›æ–¹æ³•ï¼ˆMonte Carloï¼‰

çŠ¶æ€ä»·å€¼ä¼°è®¡ï¼š
$$
V(s) = \frac{1}{N(s)} \sum_{i=1}^{N(s)} G_t^{(i)}
$$

åŠ¨ä½œä»·å€¼ä¼°è®¡ï¼š
$$
Q(s,a) = \frac{1}{N(s,a)} \sum_{i=1}^{N(s,a)} G_t^{(i)}
$$

---

## å››ã€æ—¶åºå·®åˆ†ï¼ˆTD Learningï¼‰

TD æ›´æ–°ï¼š
$$
V(s_t) \leftarrow V(s_t) + \alpha [ r_{t+1} + \gamma V(s_{t+1}) - V(s_t) ]
$$

TD è¯¯å·®ï¼š
$$
\delta_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t)
$$

---

## äº”ã€SARSAï¼ˆOn-policyï¼‰

$$
Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha [ r_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t,a_t) ]
$$

ç‰¹ç‚¹ï¼š**on-policy**ï¼Œä½¿ç”¨æ‰§è¡Œçš„åŠ¨ä½œè¿›è¡Œæ›´æ–°ã€‚

---

## å…­ã€Q-learningï¼ˆOff-policyï¼‰

$$
Q(s_t,a_t) \leftarrow Q(s_t,a_t) + \alpha [ r_{t+1} + \gamma \max_{a'} Q(s_{t+1}, a') - Q(s_t,a_t) ]
$$

ç‰¹ç‚¹ï¼š**off-policy**ï¼Œä½¿ç”¨æœ€ä¼˜åŠ¨ä½œæ›´æ–°ç›®æ ‡ã€‚

---

## ä¸ƒã€DQNï¼ˆDeep Q-Networkï¼‰

Q-learning çš„æ·±åº¦ç‰ˆæœ¬ï¼Œç”¨ç¥ç»ç½‘ç»œè¿‘ä¼¼ $Q(s,a)$ã€‚

**ç›®æ ‡å‡½æ•°ï¼š**
$$
L(\theta) = \mathbb{E}_{(s,a,r,s')} \Big[ (y_t - Q(s,a;\theta))^2 \Big]
$$

**ç›®æ ‡å€¼ï¼ˆTargetï¼‰ï¼š**
$$
y_t = r + \gamma \max_{a'} Q(s', a'; \theta^-)
$$

å…¶ä¸­ï¼š
- $\theta$ï¼šå½“å‰ç½‘ç»œå‚æ•°  
- $\theta^-$ï¼šç›®æ ‡ç½‘ç»œå‚æ•°ï¼ˆå®šæœŸå¤åˆ¶ï¼‰  

**å‚æ•°æ›´æ–°ï¼š**
$$
\theta \leftarrow \theta - \alpha \nabla_\theta L(\theta)
$$

**å…³é”®æŠ€æœ¯ï¼š**
1. ç»éªŒå›æ”¾ï¼ˆExperience Replayï¼‰
   $$\mathcal{D} = \{ (s,a,r,s') \}$$  
   ä» $\mathcal{D}$ ä¸­éšæœºé‡‡æ ·ä»¥æ‰“ç ´ç›¸å…³æ€§ã€‚  
2. ç›®æ ‡ç½‘ç»œï¼ˆTarget Networkï¼‰  
   å›ºå®š $\theta^-$ ä¸€æ®µæ—¶é—´ï¼Œä½¿ç›®æ ‡æ›´ç¨³å®šã€‚  

---

## å…«ã€ç­–ç•¥æ¢¯åº¦ï¼ˆPolicy Gradientï¼‰

ç›®æ ‡ï¼š
$$
J(\theta) = \mathbb{E}_{\pi_\theta} [ G_t ]
$$

æ¢¯åº¦ï¼š
$$
\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} [ \nabla_\theta \log \pi_\theta(a_t|s_t) G_t ]
$$

æ›´æ–°ï¼š
$$
\theta \leftarrow \theta + \alpha \, \nabla_\theta \log \pi_\theta(a_t|s_t) G_t
$$

---

## ä¹ã€REINFORCE

æŸå¤±å‡½æ•°ï¼š
$$
L(\theta) = -\mathbb{E}_{\pi_\theta} [ \log \pi_\theta(a_t|s_t) G_t ]
$$

å¸¦ baselineï¼ˆé™ä½æ–¹å·®ï¼‰ï¼š
$$
L(\theta) = -\mathbb{E}_{\pi_\theta} [ \log \pi_\theta(a_t|s_t) (G_t - b(s_t)) ]
$$

---

## åã€Actorâ€“Criticï¼ˆACï¼‰

Actorï¼ˆç­–ç•¥ï¼‰æ›´æ–°ï¼š
$$
\theta \leftarrow \theta + \alpha \, \nabla_\theta \log \pi_\theta(a_t|s_t) \, A_t
$$

Criticï¼ˆä»·å€¼å‡½æ•°ï¼‰æ›´æ–°ï¼š
$$
\phi \leftarrow \phi - \beta \, \nabla_\phi [ \delta_t^2 ]
$$

å…¶ä¸­ï¼š
$$
A_t = r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t)
$$

---

## åä¸€ã€PPOï¼ˆProximal Policy Optimizationï¼‰

**æ¦‚ç‡æ¯”ï¼š**
$$
r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_\text{old}}(a_t|s_t)}
$$

**å‰ªåˆ‡ç›®æ ‡å‡½æ•°ï¼š**
$$
L^{CLIP}(\theta) = \mathbb{E}_t \Big[
\min \big( r_t(\theta) A_t,\; \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t \big)
\Big]
$$

**å®Œæ•´æŸå¤±ï¼š**
$$
L^{PPO}(\theta) = \mathbb{E}_t \Big[
L^{CLIP}(\theta)
- c_1 (V_\theta(s_t) - V_t^{target})^2
+ c_2 H[\pi_\theta](s_t)
\Big]
$$

---

## åäºŒã€ä¼˜åŠ¿å‡½æ•°ä¸ GAE

ä¼˜åŠ¿å‡½æ•°ï¼š
$$
A_t = Q(s_t,a_t) - V(s_t)
$$

GAEï¼ˆGeneralized Advantage Estimationï¼‰ï¼š
$$
A_t^{GAE(\lambda)} = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}
$$

å…¶ä¸­ï¼š
$$
\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)
$$

---

## âœ… æ€»ç»“å±‚æ¬¡ç»“æ„

| ç±»å‹ | ç®—æ³• | è¯´æ˜ |
|------|------|------|
| **åŸºäºå€¼** | DP, MC, TD | ä¼°è®¡ $V(s)$ |
| **æ§åˆ¶ç±»** | SARSA, Q-learning, DQN | å­¦ä¹ æœ€ä¼˜ç­–ç•¥ |
| **åŸºäºç­–ç•¥** | REINFORCE, Actor-Critic, PPO | ç›´æ¥ä¼˜åŒ– $\pi_\theta$ |
| **æ··åˆå‹** | A2C, A3C, PPO | ç»“åˆå€¼ä¸ç­–ç•¥ç¨³å®šè®­ç»ƒ |

---

