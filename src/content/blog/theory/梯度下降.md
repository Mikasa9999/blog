---
title: "ä¼˜åŒ–ç®—æ³•æ¼”åŒ–ï¼šä» SGD åˆ° AdamW çš„ç³»ç»Ÿå¯¹æ¯”"
description: "æ·±å…¥å¯¹æ¯” SGDã€Momentumã€AdaGradã€RMSPropã€Adamã€AdamW ç­‰ä¼˜åŒ–ç®—æ³•çš„æ ¸å¿ƒæ€æƒ³ä¸å…¬å¼æ¼”åŒ–ï¼Œå¸®åŠ©ç†è§£ç°ä»£æ·±åº¦å­¦ä¹ ä¼˜åŒ–å™¨çš„åŸç†ã€‚"
pubDate: 2024-10-02
category: theory
tags: ["optimization", "gradient-descent", "momentum", "adaptive-optimizers", "adamw"]
draft: false
---

# ğŸš€ ä¼˜åŒ–ç®—æ³•æ¼”åŒ–ï¼šä» SGD åˆ° AdamW çš„ç³»ç»Ÿå¯¹æ¯”

ä¸‹é¢æˆ‘ä»¬ç³»ç»Ÿå¯¹æ¯” **SGD â†’ Momentum â†’ AdaGrad â†’ RMSProp â†’ Adam â†’ AdamW** è¿™å‡ ç§å¸¸è§ä¼˜åŒ–ç®—æ³•çš„æ ¸å¿ƒæ€æƒ³ä¸å…¬å¼ï¼Œè®©ä½ ä¸€çœ¼çœ‹å‡ºæ¼”åŒ–é€»è¾‘ã€‚

---

## ğŸ§© ä¸€ã€æ ‡å‡† SGDï¼ˆéšæœºæ¢¯åº¦ä¸‹é™ï¼‰

**æ€æƒ³ï¼š**  
ç›´æ¥ç”¨å½“å‰æ¢¯åº¦æ›´æ–°å‚æ•°ã€‚

$$
\theta_t = \theta_{t-1} - \eta \cdot g_t
$$

å…¶ä¸­  
$g_t = \nabla_\theta L_t(\theta_{t-1})$

**ç¼ºç‚¹ï¼š**
- æ¢¯åº¦éœ‡è¡å¤§ï¼›
- å¯¹ä¸åŒç»´åº¦çš„å­¦ä¹ ç‡ä¸æ•æ„Ÿã€‚

---

## âš™ï¸ äºŒã€Momentumï¼ˆåŠ¨é‡æ³•ï¼‰

**æ€æƒ³ï¼š**  
æ¨¡ä»¿â€œæƒ¯æ€§â€ï¼Œå¹³æ»‘æ¢¯åº¦æ–¹å‘ï¼Œé˜²æ­¢éœ‡è¡ã€‚

$$
\begin{aligned}
v_t &= \beta v_{t-1} + (1 - \beta) g_t \\
\theta_t &= \theta_{t-1} - \eta v_t
\end{aligned}
$$

**ä¼˜ç‚¹ï¼š**
- æ–¹å‘æ›´ç¨³å®šï¼ŒåŠ å¿«æ”¶æ•›é€Ÿåº¦  
**ç¼ºç‚¹ï¼š**
- å¯¹æ¯ä¸ªå‚æ•°ä½¿ç”¨ç›¸åŒå­¦ä¹ ç‡ï¼Œä»ä¸è‡ªé€‚åº”ã€‚

---

## âš¡ ä¸‰ã€AdaGradï¼ˆè‡ªé€‚åº”æ¢¯åº¦æ³•ï¼‰

**æ€æƒ³ï¼š**  
æ¯ä¸ªå‚æ•°éƒ½æœ‰ç‹¬ç«‹å­¦ä¹ ç‡ï¼›å†å²å¹³æ–¹æ¢¯åº¦è¶Šå¤§ï¼Œæ­¥é•¿è¶Šå°ã€‚

$$
\begin{aligned}
r_t &= r_{t-1} + g_t^2 \\
\theta_t &= \theta_{t-1} - \frac{\eta}{\sqrt{r_t} + \epsilon} \cdot g_t
\end{aligned}
$$

**ä¼˜ç‚¹ï¼š**
- é€‚åˆç¨€ç–æ•°æ®ï¼ˆå¦‚ NLP çš„ Embeddingï¼‰  
**ç¼ºç‚¹ï¼š**
- åˆ†æ¯ä¸æ–­ç´¯ç§¯ â†’ å­¦ä¹ ç‡ä¸æ–­å˜å° â†’ è®­ç»ƒæ—©åœã€‚

---

## ğŸ”„ å››ã€RMSPropï¼ˆå‡æ–¹æ ¹ä¼ æ’­ï¼‰

**æ€æƒ³ï¼š**  
ä¿®å¤ AdaGrad çš„å­¦ä¹ ç‡é€’å‡é—®é¢˜ï¼Œç”¨æŒ‡æ•°åŠ æƒå¹³å‡æ›¿ä»£ç´¯ç§¯ã€‚

$$
\begin{aligned}
r_t &= \beta r_{t-1} + (1 - \beta) g_t^2 \\
\theta_t &= \theta_{t-1} - \frac{\eta}{\sqrt{r_t} + \epsilon} \cdot g_t
\end{aligned}
$$

**ä¼˜ç‚¹ï¼š**
- èƒ½é•¿æœŸè®­ç»ƒï¼Œå­¦ä¹ ç‡åŠ¨æ€å¹³è¡¡  
**ç¼ºç‚¹ï¼š**
- ä»ç„¶åªçœ‹æ¢¯åº¦å¤§å°ï¼Œæ²¡æœ‰åŠ¨é‡æ–¹å‘ã€‚

---

## ğŸ§  äº”ã€Adamï¼ˆAdaptive Moment Estimationï¼‰

**æ€æƒ³ï¼š**  
ç»“åˆ Momentum + RMSPropï¼šåŒæ—¶è¿½è¸ªä¸€é˜¶åŠ¨é‡ä¸äºŒé˜¶åŠ¨é‡ã€‚

$$
\begin{aligned}
m_t &= \beta_1 m_{t-1} + (1 - \beta_1) g_t \quad &\text{ï¼ˆå¹³æ»‘æ¢¯åº¦ï¼‰}\\
v_t &= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \quad &\text{ï¼ˆå¹³æ»‘å¹³æ–¹æ¢¯åº¦ï¼‰}\\
\hat{m_t} &= \frac{m_t}{1 - \beta_1^t}, \quad \hat{v_t} = \frac{v_t}{1 - \beta_2^t} \\
\theta_t &= \theta_{t-1} - \eta \frac{\hat{m_t}}{\sqrt{\hat{v_t}} + \epsilon}
\end{aligned}
$$

**ä¼˜ç‚¹ï¼š**
- è®­ç»ƒç¨³å®šã€é€‚åº”æ€§å¼ºã€å‡ ä¹æ— éœ€è°ƒå‚  
**ç¼ºç‚¹ï¼š**
- å®¹æ˜“è¿‡æ‹Ÿåˆã€æƒé‡è¡°å‡å®ç°ä¸ç†æƒ³ï¼ˆè§ AdamWï¼‰

---

## ğŸ§© å…­ã€AdamWï¼ˆæƒé‡è¡°å‡ä¿®æ­£ï¼‰

**æ€æƒ³ï¼š**  
å°† Adam çš„ L2 æ­£åˆ™é¡¹ä¸æ¢¯åº¦æ›´æ–°å½»åº•åˆ†ç¦»ï¼Œæé«˜æ³›åŒ–æ€§ã€‚

$$
\begin{aligned}
m_t &= \beta_1 m_{t-1} + (1 - \beta_1) g_t \\
v_t &= \beta_2 v_{t-1} + (1 - \beta_2) g_t^2 \\
\hat{m_t} &= \frac{m_t}{1 - \beta_1^t}, \quad \hat{v_t} = \frac{v_t}{1 - \beta_2^t} \\
\theta_t &= \theta_{t-1} - \eta \left( \frac{\hat{m_t}}{\sqrt{\hat{v_t}} + \epsilon} + \lambda \theta_{t-1} \right)
\end{aligned}
$$

**ä¼˜ç‚¹ï¼š**
- æƒé‡è¡°å‡ç‹¬ç«‹å¤„ç†ï¼Œé¿å…è¯¯å½’ä¸€åŒ–  
- æ˜¯ Transformer / BERT / GPT ç­‰é»˜è®¤ä¼˜åŒ–å™¨  

---

## ğŸ§® ä¸ƒã€å¯¹æ¯”æ€»ç»“è¡¨

| ä¼˜åŒ–å™¨ | ä¸€é˜¶åŠ¨é‡ | äºŒé˜¶åŠ¨é‡ | å­¦ä¹ ç‡è‡ªé€‚åº” | æƒé‡è¡°å‡åˆ†ç¦» | ä¼˜ç‚¹ | ç¼ºç‚¹ |
|:--:|:--:|:--:|:--:|:--:|:--|:--|
| **SGD** | âŒ | âŒ | âŒ | âŒ | ç®€å•ç¨³å®š | æ…¢ã€æ˜“éœ‡è¡ |
| **Momentum** | âœ… | âŒ | âŒ | âŒ | å¹³æ»‘æ¢¯åº¦ï¼ŒåŠ é€Ÿæ”¶æ•› | å­¦ä¹ ç‡ç»Ÿä¸€ |
| **AdaGrad** | âŒ | âœ…(ç´¯ç§¯) | âœ… | âŒ | ç¨€ç–æ•°æ®å¥½ | å­¦ä¹ ç‡ä¼šè¡°å‡è‡³é›¶ |
| **RMSProp** | âŒ | âœ…(æ»‘åŠ¨å¹³å‡) | âœ… | âŒ | ç¨³å®šæ€§å¥½ | æ— åŠ¨é‡æ–¹å‘ |
| **Adam** | âœ… | âœ… | âœ… | âŒ | æ”¶æ•›å¿«ã€é²æ£’æ€§å¼º | å¯èƒ½è¿‡æ‹Ÿåˆ |
| **AdamW** | âœ… | âœ… | âœ… | âœ… | æ³›åŒ–æ€§å¼ºï¼Œä¸»æµæ–¹æ³• | ç¨å¤æ‚ |

---

## ğŸ“Š ç›´è§‚æ€»ç»“

